{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHCSxdfnsD6e"
   },
   "source": [
    "# ***Import Needed Libraries*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Fv3H_8PTPbTk",
    "outputId": "b8738a97-048c-4807-9b90-e33ef0dd7c43"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cufflinks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f0c35f1ce5cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcufflinks\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_notebook_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cufflinks'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re,json,nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "import seaborn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly as py\n",
    "import cufflinks as cf\n",
    "from plotly.offline import iplot\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_tvDPt2sajq"
   },
   "source": [
    "# ***Load Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3bKUclNPoUp"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('Conversation.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "1AFn8VW8RWvR",
    "outputId": "bfa6f312-b0b5-411f-d858-c9d2848eea1c"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ROHJhVNRd26",
    "outputId": "22c34a0a-0259-4a06-8f24-f5fad1c353e5"
   },
   "outputs": [],
   "source": [
    "print(\"Total sentiment:\",len(data),\n",
    "      \"\\nTotal pos sentiment:\",len(data[data.Sentiment =='positive']),\n",
    "      \"\\nTotal neg sentiment:\",len(data[data.Sentiment=='negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "-ry2nCpCRt2Y",
    "outputId": "fff4c030-e71f-4d76-ed7c-0e2de97f52b7"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "data['Sentiment'].value_counts().plot(kind='barh', figsize=(6, 4))\n",
    "plt.xlabel(\"Number of data\", labelpad=12)\n",
    "plt.ylabel(\"Sentiment Class\", labelpad=12)\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title(\"Dataset Distribution\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIqfbhlMtNEa"
   },
   "source": [
    "# ***Data Preparation And Cleaning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hW-95OVdSGK",
    "outputId": "920e05c1-c698-480a-af5e-106f10a53f93"
   },
   "outputs": [],
   "source": [
    "sample_data = [10,52,1136,1138]\n",
    "for i in sample_data:\n",
    "      print(data.Conversation[i],'\\n','Sentiment:-- ',data.Sentiment[i],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr4ibElTuKR3"
   },
   "source": [
    "**Define functions for data cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehyJdMlQdTzX"
   },
   "outputs": [],
   "source": [
    "def process_conversations(Conversation): \n",
    "    stp = open('bangla_stopwords.txt','r',encoding=\"utf8\").read().split()\n",
    "    result = Conversation.split()\n",
    "    Conversation = [word.strip() for word in result if word not in stp ]\n",
    "    Conversation =\" \".join(Conversation)\n",
    "    Conversation = re.sub('[^\\u0980-\\u09FF]',' ',str(Conversation))\n",
    "    return Conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcHt_pMpe58g",
    "outputId": "6f8251ca-1e9d-4574-d281-c8b9717964b8"
   },
   "outputs": [],
   "source": [
    "data['cleaned'] = data['Conversation'].apply(process_conversations)  \n",
    "sample_data = [10,150,1140]\n",
    "for i in sample_data:\n",
    "     print('Original:\\n',data.Conversation[i],'\\nCleaned:\\n',\n",
    "           data.cleaned[i],'\\n','Sentiment:-- ',data.Sentiment[i],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aD5OlQoze_CJ",
    "outputId": "921611aa-9811-4774-822f-f6918da31bd7"
   },
   "outputs": [],
   "source": [
    "data['length'] = data['cleaned'].apply(lambda x:len(x.split()))\n",
    "\n",
    "dataset = data.loc[data.length>0]\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "print(\"After Cleaning:\",\"\\nRemoved {} Small conversations\".format(len(data)-len(dataset)),\n",
    "      \"\\nTotal conversations:\",len(dataset))\n",
    "\n",
    "dataset['no_char'] = data['cleaned'].apply(lambda cleaned: len(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "SM28nhp5Nvgx",
    "outputId": "38154438-c062-4d88-c478-8bd42036af6b"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kqv8z3iwRz2"
   },
   "source": [
    "# ***Data Visualization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "V3wYj_UENvgy",
    "outputId": "38060119-4b4d-49d7-d95a-aa97fdde8814"
   },
   "outputs": [],
   "source": [
    "dataset['Sentiment'].value_counts().iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "VHRSoMBiNvgy",
    "outputId": "32cf2954-68db-4033-f06e-06adc286abaf"
   },
   "outputs": [],
   "source": [
    "dataset['length'].iplot(colors='orange',xTitle ='No. Conversation',yTitle ='No. Words',title='Word Frequency ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "-UDNX_G7Nvgz",
    "outputId": "c60c3c6a-9636-48b2-ff96-1f43c5495ad0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['no_char'].iplot(colors='orange',kind='bar',xTitle ='No. Conversation',yTitle ='No. Character',title='Character Frequency ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "2q00L8YigQBO",
    "outputId": "e4c399fe-b536-4650-f4a1-8d8e35bf0425"
   },
   "outputs": [],
   "source": [
    "frequency = dict()\n",
    "for i in dataset.length:\n",
    "    frequency[i] = frequency.get(i, 0)+1\n",
    "\n",
    "plt.bar(frequency.keys(), frequency.values(), color =\"red\")\n",
    "plt.xlim(1, 20)\n",
    "\n",
    "plt.xlabel('Lenght of the word')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Length-Frequency Distribution')\n",
    "plt.show()  \n",
    "print(f\"Maximum Length of a review: {max(dataset.length)}\")\n",
    "print(f\"Minimum Length of a review: {min(dataset.length)}\")\n",
    "print(f\"Average Length of a reviews: {round(np.mean(dataset.length),0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "N-qfN9v99AvQ",
    "outputId": "532b70e7-49d5-478b-f477-58acc46ba9ab"
   },
   "outputs": [],
   "source": [
    "frequency = dict()\n",
    "for i in dataset.no_char:\n",
    "    frequency[i] = frequency.get(i, 0)+1\n",
    "\n",
    "plt.bar(frequency.keys(), frequency.values(), color =\"red\")\n",
    "plt.xlim(1, 100)\n",
    "\n",
    "plt.xlabel('Lenght of the character')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Length-Frequency Distribution')\n",
    "plt.show()  \n",
    "print(f\"Maximum Length of a review: {max(dataset.no_char)}\")\n",
    "print(f\"Minimum Length of a review: {min(dataset.no_char)}\")\n",
    "print(f\"Average Length of a reviews: {round(np.mean(dataset.no_char),0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofvoKefbw9xo"
   },
   "source": [
    "***Unigram Distribution***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ep-Py9TNvg2"
   },
   "outputs": [],
   "source": [
    "x = ['বুথ কল দেয়ার দুঃখিত']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WanNFAbXNvg3",
    "outputId": "0a378873-5e46-414b-8642-4f433f7fcf96"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(x, n):\n",
    "    vec = CountVectorizer().fit(x)\n",
    "    bow = vec.transform(x)\n",
    "    sum_words = bow.sum(axis = 0)\n",
    "    words_freq = [(word, sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "get_top_n_words(x,3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IiMMtA0NNvg5",
    "outputId": "469ab984-1a2c-400d-94af-3810baad7de6"
   },
   "outputs": [],
   "source": [
    "unigram_words = get_top_n_words(data['cleaned'],20)\n",
    "unigram_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "1i2XVpXDNvg7",
    "outputId": "cbf2cc14-42f5-4ca5-c108-a5cb2d34e3c1"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(unigram_words, columns = ['Unigram', 'Frequency'])\n",
    "df = df.set_index('Unigram')\n",
    "df.iplot(kind='bar', xTitle = 'Unigram', yTitle = 'Count',title = 'Top 20 Unigram Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO2U8D00yNqY"
   },
   "source": [
    "***Bigram Distribution***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws5s8IzBNvg8"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(x, n):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(x)\n",
    "    bow = vec.transform(x)\n",
    "    sum_words = bow.sum(axis = 0)\n",
    "    words_freq = [(word, sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    return words_freq[:n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkPFZ1NaNvg9",
    "outputId": "5cc8c8ae-3413-4017-e77a-3516ce219fd4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigram_words = get_top_n_words(data['cleaned'],20)\n",
    "bigram_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "vrP47tSzNvg9",
    "outputId": "b89f62fb-dd85-4464-d031-a836313a0688"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(bigram_words, columns = ['Bigram', 'Frequency'])\n",
    "df1 = df1.set_index('Bigram')\n",
    "df1.iplot(kind='bar', xTitle = 'Bigram', yTitle = 'Count',title = 'Top 20 Bigram Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85UfWlkJyrRp"
   },
   "source": [
    " ***Trigram Distribution***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Uh9ywfNvg_"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(x, n):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3)).fit(x)\n",
    "    bow = vec.transform(x)\n",
    "    sum_words = bow.sum(axis = 0)\n",
    "    words_freq = [(word, sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    return words_freq[:n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spsmokjoNvg_",
    "outputId": "a8118c03-6de0-4b69-efe5-fe3e474a9129"
   },
   "outputs": [],
   "source": [
    "trigram_words = get_top_n_words(data['cleaned'],20)\n",
    "trigram_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "HAmzXPxONvhA",
    "outputId": "eab2bd6a-23e3-4e8b-c297-6fe79486cf20"
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(trigram_words, columns = ['Trigram', 'Frequency'])\n",
    "df2 = df2.set_index('Trigram')\n",
    "df2.iplot( kind = 'bar',xTitle = 'Trigram', yTitle = 'Count',title = 'Top 20 Trigram Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKbRQSmz0I8l"
   },
   "source": [
    "# ***Dataset Summary***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkec5o_1fXyV",
    "outputId": "7ed45a1c-9381-4a77-e861-acbb65b87c9f"
   },
   "outputs": [],
   "source": [
    "def data_summary(dataset):\n",
    "    documents = []\n",
    "    words = []\n",
    "    most_frequent_word =[]\n",
    "    u_words = []\n",
    "   \n",
    "    total_u_words = [word.strip().lower() for t in list(dataset.cleaned) for word in t.strip().split()]\n",
    "    class_label= [k for k,v in dataset.Sentiment.value_counts().to_dict().items()]\n",
    "\n",
    "    for label in class_label: \n",
    "        word_list = [word.strip().lower() for t in list(dataset[dataset.Sentiment==label].cleaned) for word in t.strip().split()]\n",
    "        \n",
    "        counts = dict()\n",
    "        \n",
    "        for word in word_list:\n",
    "                counts[word] = counts.get(word, 0)+1\n",
    "      \n",
    "        ordered = sorted(counts.items(), key= lambda item: item[1],reverse = True)\n",
    "    \n",
    "        documents.append(len(list(dataset[dataset.Sentiment==label].cleaned)))\n",
    "        \n",
    "        words.append(len(word_list))\n",
    "         \n",
    "        u_words.append(len(np.unique(word_list)))\n",
    "       \n",
    "        print(\"\\nClass Name : \",label)\n",
    "        print(\"Number of Documents:{}\".format(len(list(dataset[dataset.Sentiment==label].cleaned))))  \n",
    "        print(\"Number of Words:{}\".format(len(word_list))) \n",
    "        print(\"Number of Unique Words:{}\".format(len(np.unique(word_list)))) \n",
    "        print(\"Most Frequent Words:\\n\")\n",
    "        for k,v in ordered[:15]:\n",
    "              print(\"{}\\t{}\".format(k,v))\n",
    "    print(\"Total Number of Unique Words:{}\".format(len(np.unique(total_u_words))))           \n",
    "   \n",
    "    return documents,words,u_words,class_label\n",
    "\n",
    "documents,words,u_words,class_names = data_summary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RDfX48C0-5F"
   },
   "source": [
    "***New Dataframe for Data Summary***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKIVfVjnNvhE",
    "outputId": "d5f56bc2-a65e-43dd-da41-f5870446e667"
   },
   "outputs": [],
   "source": [
    "data_matrix = pd.DataFrame({'Total Documents':documents,\n",
    "                            'Total Words':words,\n",
    "                            'Unique Words':u_words,\n",
    "                            'Class Names':class_names})\n",
    "df = pd.melt(data_matrix, id_vars=\"Class Names\", var_name=\"Category\", value_name=\"Values\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMPVD2Mm1nx6"
   },
   "source": [
    "***Data Summary Visualization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "lBwUS8JSNvhF",
    "outputId": "85871a0c-09de-4889-ac6c-b1f8fc842a88"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot()\n",
    "\n",
    "sns.barplot(data=df,x='Class Names', y='Values' ,hue='Category')\n",
    "ax.set_xlabel('Class Names') \n",
    "ax.set_title('Data Statistics')\n",
    "\n",
    "ax.xaxis.set_ticklabels(class_names, rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "i6SmyaU0zIfa",
    "outputId": "fb6100c3-a30b-40fc-82a3-e012175cc240"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize =(6, 6))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(data_matrix.iloc[:,0:3], annot=True,fmt=\"d\", linewidths=0.5,linecolor = 'Black',cmap = \"YlGnBu\",ax = ax)\n",
    "\n",
    "ax.set_ylabel('Class Names') \n",
    "ax.set_title('Data Statistics')\n",
    "y_label = [\"Total Documents\", \"Total Words\", \"Unique Words\"] \n",
    "ax.xaxis.set_ticklabels(y_label, rotation=45); ax.yaxis.set_ticklabels(class_names, rotation=45);\n",
    "ax.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHJVZoVd9YnV"
   },
   "source": [
    "***Label Encoding Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ki7SN2qkgasW"
   },
   "outputs": [],
   "source": [
    "def label_encoding(sentiment,bool):\n",
    "  \n",
    "    le = LabelEncoder()\n",
    "    le.fit(sentiment)\n",
    "    encoded_labels = le.transform(sentiment)\n",
    "    labels = np.array(encoded_labels)\n",
    "    class_names =le.classes_\n",
    "    if bool == True:\n",
    "        print(\"\\n\\t\\t\\t===== Label Encoding =====\",\"\\nClass Names:-->\",le.classes_)\n",
    "        for i in sample_data:\n",
    "            print(sentiment[i],' ', encoded_labels[i],'\\n')\n",
    "\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gOCPSwz9pdU"
   },
   "source": [
    "***Dataset Splitting Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNVC9aV-NvhL"
   },
   "outputs": [],
   "source": [
    "def dataset_split(feature_space,sentiment):\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(feature_space,sentiment,train_size = 0.8,\n",
    "                                                  test_size = 0.2,random_state =0)\n",
    "    print(\"Feature Size :======>\",X_train.shape[1])\n",
    "    print(\"\\nDataset Distribution:\\n\")\n",
    "    print(\"\\tSet Name\",\"\\t\\tSize\")\n",
    "    print(\"\\t========\\t\\t======\")\n",
    "\n",
    "    print(\"\\tFull\\t\\t\\t\",feature_space.shape[0],\n",
    "        \"\\n\\tTraining\\t\\t\",X_train.shape[0],\n",
    "        \"\\n\\tTest\\t\\t\\t\",X_test.shape[0])\n",
    "  \n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uOgWYXnNvhN"
   },
   "outputs": [],
   "source": [
    "dataset.cleaned = dataset.cleaned.apply(lambda x:x.replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDFsV-Ie99Zo"
   },
   "source": [
    " ***Unigram Tf-idf value calculation ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vY1wdiylNvhN"
   },
   "outputs": [],
   "source": [
    "def calc_gram_tfidf(Conversations):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,3),use_idf=True,tokenizer=lambda x: x.split()) \n",
    "    X = tfidf.fit_transform(Conversations)\n",
    "    \n",
    "    return tfidf,X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6UzsfXn-PXZ"
   },
   "source": [
    "# ***Importing ML Classifiers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMvSY1OdNvhO"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBI2373g-gHC"
   },
   "source": [
    "***Classifiers Defination***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyLAd_99KrFk"
   },
   "outputs": [],
   "source": [
    "def ml_models_defination(): \n",
    "    \n",
    "    lr_model = LogisticRegression()\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    rf_model = RandomForestClassifier()\n",
    "    mnb_model = MultinomialNB(alpha=0.12)\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "    svm_model = SVC(kernel = 'linear', C=1.01, degree=1, gamma='auto')\n",
    "    sgd_model = SGDClassifier(loss = 'log',penalty='l2', max_iter=5)\n",
    "    model_names = ['Logistic Regression','Decision Tree','Random Forest','Naive Bayes','KNN','SVM','sgd']\n",
    "  \n",
    "    ml_models = [lr_model,dt_model,rf_model,mnb_model,knn_model,svm_model,sgd_model]\n",
    "\n",
    "    return ml_models,model_names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf0VR0sY-sZn"
   },
   "source": [
    "***Model Evaluation Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6GOwLkkNvhR"
   },
   "outputs": [],
   "source": [
    "def model_performace(model,X_train,X_test,y_train,y_test):\n",
    "    \n",
    "    my_dict = {}\n",
    "    model.fit(X_train,y_train)\n",
    " \n",
    "    pred_y = model.predict(X_test)\n",
    "    my_dict['Accuracy'] = round(accuracy_score(y_test, pred_y),4)*100\n",
    "    my_dict['Precision'] = round(precision_score(y_test, pred_y),4)*100 \n",
    "    my_dict['Recall'] = round(recall_score(y_test, pred_y),4)*100 \n",
    "    my_dict['F1 Score'] = round(f1_score(y_test, pred_y),4)*100\n",
    "\n",
    "    return my_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epcUk45p-5wu"
   },
   "source": [
    "***Model Performane into Dataframe***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gkNWZasNvhS"
   },
   "outputs": [],
   "source": [
    "def performance_table(performance_dict):\n",
    "\n",
    "    acc_list = []\n",
    "    pr_list = []\n",
    "    re_list = []\n",
    "    f1_list = []\n",
    "    \n",
    "    for i in performance_dict.keys():\n",
    "        acc_list.append(performance_dict[i]['Accuracy'])\n",
    "        pr_list.append(performance_dict[i]['Precision'])\n",
    "        re_list.append(performance_dict[i]['Recall'])\n",
    "        f1_list.append(performance_dict[i]['F1 Score'])\n",
    "        \n",
    "    model_names = ['Logistic Regression','Decision Tree','Random Forest','Multi. Naive Bayes','KNN','SVM','SGD']\n",
    "    performance_df = pd.DataFrame({'Accuracy':acc_list,'Precision':pr_list,\n",
    "                                   'Recall':re_list,'F1 Score':f1_list,'Model Name':model_names })\n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii_BXcFu-_Yw"
   },
   "source": [
    "***Save the performance of the model for each gram feature***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YchG8PcsNvhS",
    "outputId": "7ee91708-b124-4035-8b67-9dc492a3aeec"
   },
   "outputs": [],
   "source": [
    "gram_names = ['Unigram','Bigram','Trigram']\n",
    "ngrams = [(1,1),(1,2),(1,3)]\n",
    "\n",
    "\n",
    "for i,gram in enumerate(ngrams):\n",
    "    tfidf,feature = calc_gram_tfidf(dataset.cleaned)\n",
    "    labels = label_encoding(dataset.Sentiment,False)\n",
    "    X_train,X_test,y_train,y_test = dataset_split(feature,labels) \n",
    "    ml_models,model_names = ml_models_defination()\n",
    "    accuracy = {f'{model_names[j]}':model_performace(model,X_train,X_test,y_train,y_test) for j,model in enumerate(ml_models)}\n",
    "\n",
    "    with open(f'ml_performance_{gram_names[i]}.json', 'w') as f:\n",
    "          json.dump(accuracy,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E55qNnUq_Ltq"
   },
   "source": [
    "***Table for Accuracy along with Precision, Recall and F1_Score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "BZGwRYSBNvhT",
    "outputId": "275ebaaf-10ff-4a74-c470-14345f09973e"
   },
   "outputs": [],
   "source": [
    "table = performance_table(accuracy)\n",
    "df3 =pd.DataFrame(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLpjLJimryHX",
    "outputId": "ae87d2bc-e3ec-4841-ea8a-0b9ba24dd5bd"
   },
   "outputs": [],
   "source": [
    "print(f\"Highest Accuracy achieved by {table.Accuracy.idxmax(axis = 0)} at = {max(table.Accuracy)}\")\n",
    "print(f\"Highest F1-Score achieved by {table['F1 Score'].idxmax(axis = 0)} at = {max(table['F1 Score'] )}\")\n",
    "print(f\"Highest Precision Score achieved by {table['Precision'].idxmax(axis = 0)} at = {max(table['Precision'] )}\")\n",
    "print(f\"Highest Recall Score achieved by {table['Recall'].idxmax(axis = 0)} at = {max(table['Recall'] )}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "4k581jCEryHX",
    "outputId": "53abf790-a985-4f3c-f21d-46db3b156b7a"
   },
   "outputs": [],
   "source": [
    "df3.iplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9BDv8Z3_sUc"
   },
   "source": [
    "***Select best classifier model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pu99gKPqNvhX",
    "outputId": "2c3e773c-887b-4e6e-d2d0-8c6732ee201f"
   },
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel = 'linear', C=1.01, degree=1, gamma='auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MldknT1e_20a"
   },
   "source": [
    "***Confusion matrix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfPWNaWnQ9cq",
    "outputId": "b31a2b81-8d85-44e4-bc50-8b82e6bcb112"
   },
   "outputs": [],
   "source": [
    "y_pred = svm_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5xph1l3AAj3"
   },
   "source": [
    "# ***Test Our Model With Random Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgLUaXrLRWh_"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('cs_svm.pkl', 'wb')\n",
    "pickle.dump(svm_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83gbQilfRrpt",
    "outputId": "25644ef6-98ed-4398-f71f-640550f63523"
   },
   "outputs": [],
   "source": [
    "model = open('cs_svm.pkl','rb')\n",
    "svm_model = pickle.load(model)\n",
    "Conversation = 'মামা কি অবস্থা আজকে তো এল ক্লাসিকো খেলা আছে কে জিতবে রিয়াল নাকি বার্সা তো খেলাটা কোন চ্যানেলে দেখাবে.... খেলা রাত 11 টায় সনি টিভিতে দেখাবে তো আমার বাসায় আসিস একসাথে দেখব আচ্ছা মামা ঠিক আছে ঝালমুড়ি রেডি রাখিস'\n",
    "processed_conversation = process_conversations(Conversation)\n",
    "if (len(processed_conversation))>0:\n",
    "    cv,feature_vector = calc_gram_tfidf(dataset.cleaned) \n",
    "    feature = cv.transform([processed_conversation]).toarray()\n",
    "    sentiment = svm_model.predict(feature)\n",
    "    if (sentiment ==0):\n",
    "        print(f\"It is a Negative conversation\")\n",
    "    else:\n",
    "        print(f\"It is a Positive conversation\")\n",
    "else:\n",
    "    print(\"This conversation doesn't contains any bengali Words, thus cannot predict the Sentiment.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Conversation_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
